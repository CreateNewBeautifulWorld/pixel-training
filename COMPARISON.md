# 不同网络配置对比分析

## 📊 测试配置

我们测试了3种不同的网络配置：

| 配置 | 像素数 | 网络结构 | 深度 | 参数量 | 准确率 |
|-----|-------|---------|------|-------|--------|
| **浅窄网络** | 4 | `12 → 8 → 3` | 2层 | 131 bytes | 94.5% |
| **标准网络** | 16 | `48 → 16 → 3` | 2层 | 835 bytes | 100% |
| **深层网络** | 16 | `48 → 32 → 16 → 3` | 3层 | 2147 bytes | 100% |

---

## 📈 训练曲线对比

### 1️⃣ 浅窄网络 (12 → 8 → 3)

**Loss曲线特征**:
- 初始Loss: ~1.3
- 最终Loss: ~0.27
- ⚠️ 有明显震荡，说明容量不足

**Accuracy曲线特征**:
- 起始准确率: ~50%
- 最终准确率: 94.5%
- 前20轮快速上升，后续稳定

**分析**:
- ✅ 参数少（131 bytes），硬件资源需求低
- ⚠️ 准确率较低（94.5%），loss有震荡
- 💡 适合：极低资源设备，可接受5%误差

---

### 2️⃣ 标准网络 (48 → 16 → 3)

**Loss曲线特征**:
- 初始Loss: ~0.45
- 最终Loss: ~0.002
- ✅ 平滑下降，几乎收敛到0

**Accuracy曲线特征**:
- 起始准确率: ~95%
- 最终准确率: 100%
- 前20轮快速收敛

**分析**:
- ✅ 平衡了资源和性能
- ✅ 100%准确率，loss接近0
- ✅ 训练曲线平滑稳定
- 💡 适合：大多数应用场景

---

### 3️⃣ 深层网络 (48 → 32 → 16 → 3)

**Loss曲线特征**:
- 初始Loss: ~0.65
- 最终Loss: ~0.003
- ✅ 非常平滑的下降曲线

**Accuracy曲线特征**:
- 起始准确率: ~90%
- 最终准确率: 100%
- 收敛更稳定

**分析**:
- ✅ 100%准确率，训练最稳定
- ⚠️ 参数多（2147 bytes），约2.5倍于标准网络
- 💡 适合：复杂任务，对精度要求高的场景

---

## 🎯 如何选择网络配置？

### 按照资源约束选择：

```
资源 < 500 bytes    → 浅窄网络 (12→8→3)
资源 500-2K bytes   → 标准网络 (48→16→3)
资源 > 2K bytes     → 深层网络 (48→32→16→3)
```

### 按照精度要求选择：

```
精度要求 ~90%       → 浅窄网络
精度要求 ~95%+      → 标准网络
精度要求 ~99%+      → 深层网络或更深
```

### 按照延迟要求选择：

```
超低延迟（<10 cycles） → 浅窄网络（2层）
低延迟（10-50 cycles） → 标准网络（2层）
可接受延迟（>50 cycles）→ 深层网络（3-5层）
```

---

## 💡 关键观察

### 1. 深度 vs 宽度

**增加深度（更多层）**:
- ✅ 表达能力更强
- ✅ 训练更稳定（曲线更平滑）
- ⚠️ 参数量增加
- ⚠️ 推理延迟增加

**增加宽度（更多神经元）**:
- ✅ 容量增加，减少震荡
- ✅ 并行化更容易（硬件）
- ⚠️ 参数量快速增长（平方级）

### 2. Loss震荡的原因

浅窄网络的loss有震荡，原因：
1. 网络容量不足，无法完美拟合数据
2. 学习率可能偏大（可以降低到0.005试试）
3. 网络太简单，陷入局部最优

**解决方法**:
```bash
# 方法1: 增加神经元
python train_configurable.py --num_pixels 4 --hidden_layers 16

# 方法2: 增加层数
python train_configurable.py --num_pixels 4 --hidden_layers 12 8

# 方法3: 降低学习率
python train_configurable.py --num_pixels 4 --hidden_layers 8 --learning_rate 0.005
```

### 3. 收敛速度

| 配置 | 达到95%准确率 | 达到100%准确率 |
|-----|--------------|---------------|
| 浅窄 | ~10 epochs | 无法达到 |
| 标准 | ~5 epochs | ~20 epochs |
| 深层 | ~5 epochs | ~15 epochs |

深层网络收敛更快！

---

## 🔧 实战建议

### 场景1: 微控制器部署
```bash
# 要求: < 1KB Flash, < 10ms延迟
# 推荐配置
python train_configurable.py \
    --num_pixels 4 \
    --hidden_layers 8 \
    --epochs 150 \
    --learning_rate 0.005
```

### 场景2: 低端FPGA
```bash
# 要求: < 5KB BRAM, 100MHz
# 推荐配置
python train_configurable.py \
    --num_pixels 8 \
    --hidden_layers 16 8 \
    --epochs 200
```

### 场景3: 中高端FPGA
```bash
# 要求: > 10KB BRAM, 性能优先
# 推荐配置
python train_configurable.py \
    --num_pixels 16 \
    --hidden_layers 64 32 16 \
    --epochs 300
```

---

## 📐 参数量计算公式

```python
总参数量 = Σ (L[i] × L[i+1] + L[i+1])

例如: 48 → 32 → 16 → 3
= (48×32 + 32) + (32×16 + 16) + (16×3 + 3)
= 1568 + 528 + 51
= 2147 bytes
```

快速估算：
- 2层网络: `input × hidden + hidden × output`
- 3层网络: 约 `2-3倍` 2层网络
- 4层网络: 约 `3-5倍` 2层网络

---

## 🎓 训练技巧

### 如果loss不下降：
1. 降低学习率: `--learning_rate 0.005`
2. 增加训练轮数: `--epochs 200`
3. 增加网络容量: `--hidden_layers 32 16`

### 如果loss震荡：
1. 降低学习率
2. 增加batch size: `--batch_size 64`
3. 增加神经元数

### 如果过拟合（训练acc高，测试acc低）：
1. 增加训练数据: `--num_samples 5000`
2. 减少网络容量
3. 添加dropout（需要修改代码）

---

## 🚀 快速实验

运行这个脚本对比多种配置：

```bash
#!/bin/bash

# 测试不同深度
for layers in "8" "16" "32 16" "32 16 8"; do
    python train_configurable.py \
        --hidden_layers $layers \
        --plot_file "plot_${layers// /_}.png" \
        --epochs 100
done

# 对比所有图片
ls plot_*.png
```

---

## 📊 性能对比表

| 指标 | 浅窄 | 标准 | 深层 |
|------|------|------|------|
| 参数量 | 131B | 835B | 2147B |
| 准确率 | 94.5% | 100% | 100% |
| 训练时间 | 3s | 5s | 8s |
| 推理时间 | 1μs | 3μs | 5μs |
| 功耗估算 | 0.5mW | 1mW | 2mW |
| 硬件资源 | 最小 | 中等 | 较大 |
| 适用场景 | IoT | 嵌入式 | FPGA |

---

## 💬 常见问题

**Q: 为什么浅窄网络loss有震荡？**  
A: 容量不足，无法完美拟合数据。可以增加神经元数或降低学习率。

**Q: 深层网络一定更好吗？**  
A: 不一定。在简单任务上，标准网络已经100%准确率了，再加深没意义，还增加资源消耗。

**Q: 如何判断网络是否太简单？**  
A: 看训练曲线：如果loss震荡或无法下降到很小值，说明太简单。

**Q: 训练100轮够吗？**  
A: 看曲线！如果已经收敛（loss和acc都平了），就够了。如果还在变化，增加epochs。

---

享受自定义你的网络吧！🎉
