import numpy as np
import matplotlib.pyplot as plt
import json
import os
from datetime import datetime
import itertools

class NeuralNetworkExplorer:
    """ç½‘ç»œæ¶æ„æ¢ç´¢å™¨ - FP16è®­ç»ƒ"""
    def __init__(self, layer_sizes):
        """
        å‚æ•°:
            layer_sizes: list, æ¯å±‚ç¥ç»å…ƒæ•°
            ä¾‹å¦‚: [12, 8, 4, 1] è¡¨ç¤º 12è¾“å…¥ -> 8 -> 4 -> 1è¾“å‡º
        """
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes) - 1
        
        # ä½¿ç”¨FP16åˆå§‹åŒ–æƒé‡
        self.weights = []
        self.biases = []
        
        for i in range(self.num_layers):
            # Heåˆå§‹åŒ–
            w = np.random.randn(layer_sizes[i+1], layer_sizes[i]).astype(np.float16) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros(layer_sizes[i+1], dtype=np.float16)
            self.weights.append(w)
            self.biases.append(b)
        
        # è®­ç»ƒå†å²
        self.history = {
            'loss': [],
            'val_loss': []
        }
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        return (x > 0).astype(np.float16)
    
    def forward(self, X):
        """å‰å‘ä¼ æ’­ - å›å½’ä»»åŠ¡"""
        # å½’ä¸€åŒ–è¾“å…¥åˆ°[0,1]
        activations = [X.astype(np.float16) / 255.0]
        
        for i in range(self.num_layers):
            z = np.dot(activations[-1], self.weights[i].T) + self.biases[i]
            
            if i < self.num_layers - 1:  # éšè—å±‚ç”¨ReLU
                a = self.relu(z)
            else:  # è¾“å‡ºå±‚ä¸ç”¨æ¿€æ´»ï¼ˆçº¿æ€§å›å½’ï¼‰
                a = z
            
            activations.append(a)
        
        return activations
    
    def mse_loss(self, predictions, targets):
        """å‡æ–¹è¯¯å·®æŸå¤±"""
        return np.mean((predictions - targets) ** 2)
    
    def backward(self, X, y, learning_rate=0.01):
        """åå‘ä¼ æ’­ - MSEæŸå¤±"""
        m = X.shape[0]
        
        # å‰å‘ä¼ æ’­
        activations = self.forward(X)
        predictions = activations[-1].flatten()
        
        # è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦ (MSEå¯¼æ•°)
        delta = (predictions - y).reshape(-1, 1) / m
        
        # ä¿å­˜ä¸­é—´æ¿€æ´»å€¼
        z_values = []
        for i in range(self.num_layers):
            z = np.dot(activations[i], self.weights[i].T) + self.biases[i]
            z_values.append(z)
        
        # åå‘ä¼ æ’­
        for i in range(self.num_layers - 1, -1, -1):
            # è®¡ç®—æ¢¯åº¦
            dW = np.dot(delta.T, activations[i])
            db = np.sum(delta, axis=0)
            
            # æ›´æ–°æƒé‡
            self.weights[i] = (self.weights[i] - learning_rate * dW).astype(np.float16)
            self.biases[i] = (self.biases[i] - learning_rate * db).astype(np.float16)
            
            # ä¼ æ’­åˆ°å‰ä¸€å±‚
            if i > 0:
                delta = np.dot(delta, self.weights[i]) * self.relu_derivative(z_values[i-1])
    
    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, learning_rate=0.01, verbose=False):
        """è®­ç»ƒæ¨¡å‹"""
        n_samples = X_train.shape[0]
        
        for epoch in range(epochs):
            # éšæœºæ‰“ä¹±
            indices = np.random.permutation(n_samples)
            X_shuffled = X_train[indices]
            y_shuffled = y_train[indices]
            
            # Mini-batchè®­ç»ƒ
            for i in range(0, n_samples, batch_size):
                batch_X = X_shuffled[i:i+batch_size]
                batch_y = y_shuffled[i:i+batch_size]
                
                self.backward(batch_X, batch_y, learning_rate)
            
            # è®¡ç®—è®­ç»ƒå’ŒéªŒè¯æŸå¤±
            train_pred = self.forward(X_train)[-1].flatten()
            train_loss = self.mse_loss(train_pred, y_train)
            
            val_pred = self.forward(X_val)[-1].flatten()
            val_loss = self.mse_loss(val_pred, y_val)
            
            self.history['loss'].append(float(train_loss))
            self.history['val_loss'].append(float(val_loss))
            
            if verbose and (epoch + 1) % 10 == 0:
                print(f'  Epoch [{epoch+1:3d}/{epochs}] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')
        
        return float(val_loss)  # è¿”å›æœ€ç»ˆéªŒè¯æŸå¤±
    
    def predict(self, X):
        """é¢„æµ‹"""
        activations = self.forward(X)
        return activations[-1].flatten()


def generate_regression_data(num_samples=1000, num_pixels=16, noise_level=0.1):
    """
    ç”Ÿæˆå›å½’æ•°æ®
    è¾“å‡º = åƒç´ äº®åº¦çš„æŸç§å‡½æ•° + å™ªå£°
    """
    X = np.random.randint(0, 256, size=(num_samples, num_pixels * 3), dtype=np.uint8)
    
    # ç›®æ ‡å‡½æ•°ï¼šå½’ä¸€åŒ–åçš„å¹³å‡äº®åº¦ + ä¸€äº›éçº¿æ€§
    X_normalized = X.astype(np.float32) / 255.0
    y = np.mean(X_normalized, axis=1)  # å¹³å‡äº®åº¦
    y = y ** 1.5  # éçº¿æ€§å˜æ¢
    y += np.random.randn(num_samples) * noise_level  # æ·»åŠ å™ªå£°
    y = np.clip(y, 0, 1)  # é™åˆ¶åœ¨[0,1]
    
    return X, y.astype(np.float32)


def grid_search_architecture(
    num_pixels_range=[1, 2, 4, 8, 16, 32, 64],
    hidden_layers_range=[0, 1, 2, 3, 4, 5],
    hidden_neurons_range=[4, 8, 16, 32],
    num_samples=1000,
    epochs=50,
    output_dir='exploration_results'
):
    """
    ç½‘æ ¼æœç´¢æœ€ä¼˜æ¶æ„
    
    å‚æ•°:
        num_pixels_range: æµ‹è¯•çš„åƒç´ æ•°åˆ—è¡¨
        hidden_layers_range: æµ‹è¯•çš„éšè—å±‚æ•°åˆ—è¡¨
        hidden_neurons_range: æµ‹è¯•çš„éšè—å±‚ç¥ç»å…ƒæ•°åˆ—è¡¨
        num_samples: æ¯ä¸ªé…ç½®çš„è®­ç»ƒæ ·æœ¬æ•°
        epochs: æ¯ä¸ªé…ç½®çš„è®­ç»ƒè½®æ•°
        output_dir: ç»“æœè¾“å‡ºç›®å½•
    """
    
    os.makedirs(output_dir, exist_ok=True)
    
    results = []
    total_configs = len(num_pixels_range) * len(hidden_layers_range) * len(hidden_neurons_range)
    config_idx = 0
    
    print("=" * 80)
    print("ç½‘ç»œæ¶æ„æ¢ç´¢ - Grid Search")
    print("=" * 80)
    print(f"æ€»é…ç½®æ•°: {total_configs}")
    print(f"åƒç´ æ•°èŒƒå›´: {num_pixels_range}")
    print(f"éšè—å±‚æ•°èŒƒå›´: {hidden_layers_range}")
    print(f"ç¥ç»å…ƒæ•°èŒƒå›´: {hidden_neurons_range}")
    print("=" * 80)
    
    for num_pixels in num_pixels_range:
        # ç”Ÿæˆæ•°æ®
        print(f"\nç”Ÿæˆæ•°æ®: {num_pixels} åƒç´ ...")
        X, y = generate_regression_data(num_samples=num_samples, num_pixels=num_pixels)
        
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
        split = int(0.8 * num_samples)
        X_train, X_val = X[:split], X[split:]
        y_train, y_val = y[:split], y[split:]
        
        input_dim = num_pixels * 3
        
        for num_hidden_layers in hidden_layers_range:
            for hidden_neurons in hidden_neurons_range:
                config_idx += 1
                
                # æ„å»ºç½‘ç»œç»“æ„
                if num_hidden_layers == 0:
                    # ç›´æ¥ä»è¾“å…¥åˆ°è¾“å‡º
                    layer_sizes = [input_dim, 1]
                else:
                    # æœ‰éšè—å±‚
                    layer_sizes = [input_dim] + [hidden_neurons] * num_hidden_layers + [1]
                
                # è®¡ç®—å‚æ•°é‡
                total_params = sum(
                    layer_sizes[i] * layer_sizes[i+1] + layer_sizes[i+1]
                    for i in range(len(layer_sizes) - 1)
                )
                
                print(f"\n[{config_idx}/{total_configs}] æµ‹è¯•é…ç½®:")
                print(f"  åƒç´ æ•°: {num_pixels}")
                print(f"  ç½‘ç»œç»“æ„: {' -> '.join(map(str, layer_sizes))}")
                print(f"  éšè—å±‚æ•°: {num_hidden_layers}")
                print(f"  æ¯å±‚ç¥ç»å…ƒ: {hidden_neurons if num_hidden_layers > 0 else 'N/A'}")
                print(f"  å‚æ•°é‡: {total_params}")
                
                # è®­ç»ƒæ¨¡å‹
                model = NeuralNetworkExplorer(layer_sizes)
                final_val_loss = model.train(
                    X_train, y_train, X_val, y_val,
                    epochs=epochs, batch_size=32, learning_rate=0.01,
                    verbose=False
                )
                
                final_train_loss = model.history['loss'][-1]
                
                print(f"  æœ€ç»ˆ Train Loss: {final_train_loss:.6f}")
                print(f"  æœ€ç»ˆ Val Loss: {final_val_loss:.6f}")
                
                # è®°å½•ç»“æœ
                result = {
                    'config_id': config_idx,
                    'num_pixels': num_pixels,
                    'input_dim': input_dim,
                    'num_hidden_layers': num_hidden_layers,
                    'hidden_neurons': hidden_neurons,
                    'layer_sizes': layer_sizes,
                    'total_params': total_params,
                    'final_train_loss': final_train_loss,
                    'final_val_loss': final_val_loss,
                    'train_history': model.history['loss'],
                    'val_history': model.history['val_loss']
                }
                results.append(result)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = os.path.join(output_dir, f'exploration_results_{timestamp}.json')
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return results, results_file


def analyze_results(results, output_dir='exploration_results'):
    """åˆ†ææ¢ç´¢ç»“æœ"""
    
    print("\n" + "=" * 80)
    print("ç»“æœåˆ†æ")
    print("=" * 80)
    
    # æŒ‰éªŒè¯æŸå¤±æ’åº
    sorted_results = sorted(results, key=lambda x: x['final_val_loss'])
    
    # Top 10é…ç½®
    print("\nTop 10 æœ€ä½³é…ç½® (æŒ‰éªŒè¯æŸå¤±):")
    print("-" * 80)
    for i, result in enumerate(sorted_results[:10], 1):
        print(f"\næ’å #{i}:")
        print(f"  ç½‘ç»œç»“æ„: {' -> '.join(map(str, result['layer_sizes']))}")
        print(f"  åƒç´ æ•°: {result['num_pixels']}")
        print(f"  éšè—å±‚æ•°: {result['num_hidden_layers']}")
        print(f"  ç¥ç»å…ƒæ•°: {result['hidden_neurons']}")
        print(f"  å‚æ•°é‡: {result['total_params']}")
        print(f"  éªŒè¯æŸå¤±: {result['final_val_loss']:.6f}")
    
    # å¯è§†åŒ–
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # 1. å‚æ•°é‡ vs éªŒè¯æŸå¤±
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    params = [r['total_params'] for r in results]
    val_losses = [r['final_val_loss'] for r in results]
    
    axes[0, 0].scatter(params, val_losses, alpha=0.6)
    axes[0, 0].set_xlabel('å‚æ•°é‡')
    axes[0, 0].set_ylabel('éªŒè¯æŸå¤±')
    axes[0, 0].set_title('å‚æ•°é‡ vs éªŒè¯æŸå¤±')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. éšè—å±‚æ•° vs éªŒè¯æŸå¤±
    hidden_layers = [r['num_hidden_layers'] for r in results]
    axes[0, 1].scatter(hidden_layers, val_losses, alpha=0.6)
    axes[0, 1].set_xlabel('éšè—å±‚æ•°')
    axes[0, 1].set_ylabel('éªŒè¯æŸå¤±')
    axes[0, 1].set_title('éšè—å±‚æ•° vs éªŒè¯æŸå¤±')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. è¾“å…¥ç»´åº¦ vs éªŒè¯æŸå¤±
    input_dims = [r['input_dim'] for r in results]
    axes[1, 0].scatter(input_dims, val_losses, alpha=0.6)
    axes[1, 0].set_xlabel('è¾“å…¥ç»´åº¦')
    axes[1, 0].set_ylabel('éªŒè¯æŸå¤±')
    axes[1, 0].set_title('è¾“å…¥ç»´åº¦ vs éªŒè¯æŸå¤±')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Top 5è®­ç»ƒæ›²çº¿
    for i, result in enumerate(sorted_results[:5]):
        label = f"{' -> '.join(map(str, result['layer_sizes']))}"
        axes[1, 1].plot(result['val_history'], label=label, linewidth=2)
    
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('éªŒè¯æŸå¤±')
    axes[1, 1].set_title('Top 5 é…ç½®è®­ç»ƒæ›²çº¿')
    axes[1, 1].legend(fontsize=8)
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_file = os.path.join(output_dir, 'exploration_analysis.png')
    plt.savefig(plot_file, dpi=150, bbox_inches='tight')
    print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {plot_file}")
    plt.close()
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨
    print("\nç”Ÿæˆè¯¦ç»†å¯¹æ¯”è¡¨...")
    comparison_file = os.path.join(output_dir, 'comparison_table.txt')
    with open(comparison_file, 'w') as f:
        f.write("ç½‘ç»œæ¶æ„æ¢ç´¢ - å¯¹æ¯”è¡¨\n")
        f.write("=" * 120 + "\n")
        f.write(f"{'æ’å':<6} {'ç½‘ç»œç»“æ„':<30} {'åƒç´ ':<6} {'å±‚æ•°':<6} {'ç¥ç»å…ƒ':<8} {'å‚æ•°é‡':<10} {'éªŒè¯æŸå¤±':<12}\n")
        f.write("-" * 120 + "\n")
        
        for i, result in enumerate(sorted_results, 1):
            structure = ' -> '.join(map(str, result['layer_sizes']))
            f.write(f"{i:<6} {structure:<30} {result['num_pixels']:<6} "
                   f"{result['num_hidden_layers']:<6} {result['hidden_neurons']:<8} "
                   f"{result['total_params']:<10} {result['final_val_loss']:<12.6f}\n")
    
    print(f"å¯¹æ¯”è¡¨å·²ä¿å­˜: {comparison_file}")
    
    # è¿”å›æœ€ä½³é…ç½®
    best_config = sorted_results[0]
    print("\n" + "=" * 80)
    print("ğŸ† æœ€ä½³é…ç½®:")
    print("=" * 80)
    print(f"  ç½‘ç»œç»“æ„: {' -> '.join(map(str, best_config['layer_sizes']))}")
    print(f"  åƒç´ æ•°: {best_config['num_pixels']}")
    print(f"  éšè—å±‚æ•°: {best_config['num_hidden_layers']}")
    print(f"  ç¥ç»å…ƒæ•°: {best_config['hidden_neurons']}")
    print(f"  å‚æ•°é‡: {best_config['total_params']}")
    print(f"  éªŒè¯æŸå¤±: {best_config['final_val_loss']:.6f}")
    print("=" * 80)
    
    return best_config


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ç¥ç»ç½‘ç»œæ¶æ„æ¢ç´¢')
    
    # æ¢ç´¢èŒƒå›´
    parser.add_argument('--pixels', type=int, nargs='+', default=[1, 2, 4, 8, 16],
                        help='æµ‹è¯•çš„åƒç´ æ•°åˆ—è¡¨ (é»˜è®¤: 1 2 4 8 16)')
    parser.add_argument('--layers', type=int, nargs='+', default=[0, 1, 2, 3],
                        help='æµ‹è¯•çš„éšè—å±‚æ•°åˆ—è¡¨ (é»˜è®¤: 0 1 2 3)')
    parser.add_argument('--neurons', type=int, nargs='+', default=[4, 8, 16],
                        help='æµ‹è¯•çš„ç¥ç»å…ƒæ•°åˆ—è¡¨ (é»˜è®¤: 4 8 16)')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--samples', type=int, default=1000,
                        help='æ¯ä¸ªé…ç½®çš„è®­ç»ƒæ ·æœ¬æ•° (é»˜è®¤: 1000)')
    parser.add_argument('--epochs', type=int, default=50,
                        help='æ¯ä¸ªé…ç½®çš„è®­ç»ƒè½®æ•° (é»˜è®¤: 50)')
    
    # è¾“å‡º
    parser.add_argument('--output_dir', type=str, default='exploration_results',
                        help='ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: exploration_results)')
    
    args = parser.parse_args()
    
    # è¿è¡Œæ¢ç´¢
    results, results_file = grid_search_architecture(
        num_pixels_range=args.pixels,
        hidden_layers_range=args.layers,
        hidden_neurons_range=args.neurons,
        num_samples=args.samples,
        epochs=args.epochs,
        output_dir=args.output_dir
    )
    
    # åˆ†æç»“æœ
    best_config = analyze_results(results, output_dir=args.output_dir)
    
    print("\n\næ¢ç´¢å®Œæˆ! ğŸ‰")
    print(f"ç»“æœæ–‡ä»¶: {results_file}")
    print(f"å¯è§†åŒ–å›¾: {args.output_dir}/exploration_analysis.png")
    print(f"å¯¹æ¯”è¡¨: {args.output_dir}/comparison_table.txt")
